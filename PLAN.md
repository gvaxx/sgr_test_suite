1. Ллм клиент опенаи compatible с ретраями
2. Тест раннер который берет пайплайн берет тест кейсы прогоняет их и выдает результат
3. Ui через градио
4. Cli запуск
5. Базовый класс пайплайна который на вход получает какие-то аргументы отдает какую-то схему чаще всего какой-то словарь
6. Наследники базового класса также включают в себя системные промпты схемы для so и юзер промпты для взаимодействия с llm используют клиент.
7. Тесты написаны в json схеме где у нас по сути есть id input params, expected_output
8. Структура такая
sgr/
….routing/
…….pipeline.py
…….test_cases.json
…….reports/
В репортах содержится информация о ранах в json формате. Там будет accuracy сколько токенов съел сколько по времени заняло и какая модель ее параметры и прочее.

Через ui можно посмотреть информацию о последних ранах и о лучших.
Таких схем может быть десятки сейчас есть роуминг а может быть сплиттер может быть guarding и так далее 

Используем питон через uv

Запланированные задачи

Текущие таски:

Сделанные таски:
- Создан LLM клиент совместимый с OpenAI с поддержкой ретраев.
- Тест раннер который берет пайплайн берет тест кейсы прогоняет их и выдает результат.
- UI через Gradio для запуска тестов и просмотра результатов.
